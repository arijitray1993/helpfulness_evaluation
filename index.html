<!DOCTYPE html>
<html lang="en">
<head>
    <title>Explanation Evaluation</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.16.0/umd/popper.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>
</head>

<body>
    <div class="jumbotron jumbotron-fluid">
        <div class="container-fluid">
            <h1>Knowing What VQA Doesn't: Pointing to Error-Inducing Regions to Improve Explanation Helpfulness</h1>
            <p>Arijit Ray, Michael Cogswell, Xiao Lin, Kamran Alipour, Ajay Divakaran, Yi Yao, Giedrius Burachas</p>
            
        </div> 
    </div>

    <div class='container'>

        <div class="row">
            <div class="card">
                <div class="card-header">
                    Abstract
                </div>
                <div class="card-body">
                    Attention maps, a popular heatmap-based explanation method for Visual Question Answering (VQA), 
                    are supposed to help users understand the model by highlighting portions of the image/question 
                    used by the model to infer answers. However, we see that users are often misled by current attention 
                    map visualizations that point to relevant regions despite the model producing an incorrect answer.
                    Hence, we propose Error Maps that clarify the error by highlighting image regions where the model 
                    is prone to err. Error maps can indicate when a correctly attended region may be processed incorrectly 
                    leading to an incorrect answer, and hence, improve users' understanding of those cases.To evaluate
                    our new explanations, we further introduce a metric that simulates users' interpretation of 
                    explanations to evaluate their potential helpfulness to understand model correctness. We finally
                    conduct user studies to see that our new explanations help users understand model correctness 
                    better than baselines by an expected 30% and that our proxy helpfulness metrics correlate 
                    strongly (&#961;>0.97) with how well users can predict model correctness.
                </div>
            </div>
        </div>
        
        <!--
        <div class-"row">
            <h2>High Level Summary:</h2>
            <h3>Error Maps help understand VQA performance better</h3>
            <p></p>
            <h3>We also designed a proxy metric to evaluate potential helpfulness of attention/error maps to users to predict model performance</h3>
            <p></p>
        </div>
        -->

        <div class="row">
            <div class="col">
                <div class="media border p-3">
                    <a href="https://arxiv.org/abs/2103.14712"> 
                    <img src = "paper_screen_shot.png"  class="mr-3 mt-3" style="height:200px;"> 
                    
                    <div class="media-body">
                        [ArXiv Paper]
                    </div>
                    </a>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col">
                Evaluation Server coming up! Upload your attention/error map values to evaluate the potential helpfulness score (HELP_Z).
            </div>
            
        </div>


        <div class="row">
            <a href="attention_refine.html"> Qualitative examples of our attention and error maps. </a>
        </div>


    </div>

    <hr>

</body>
    
